# Comparative-analysis-of-transformers-on-multi-label-pubmed-data
Analysis of transformer models (BERT, RoBERTa, Electra,XL-Net,ERNIE) on multi-label PubMed data. Features code, datasets, and evaluations for biomedical NLP tasks. Focuses on model performance with complex labels, providing key metrics and insights for future research in the biomedical domain

Comparative Analysis of Transformers on Multi-Label PubMed Data
Overview

This repository is dedicated to a comparative analysis of various transformer models on multi-label classification tasks, using a dataset derived from PubMed, an extensive repository of medical literature. The focus is on evaluating the effectiveness of transformer architectures like BERT, RoBERTa, and GPT in handling the complex, multi-dimensional labels typical in biomedical texts.
Contents

    Code: Scripts and notebooks for training and evaluating models.
    Datasets: Preprocessed PubMed data ready for multi-label classification.
    Documentation: Detailed guidelines and methodology descriptions.

Objective

The primary goal is to assess and compare the performance of different transformer models in the context of natural language processing within the biomedical domain. By doing so, we aim to provide insights and recommendations for future research and practical applications in this field.
Models Analyzed

    BERT (Bidirectional Encoder Representations from Transformers)
    RoBERTa (Robustly Optimized BERT approach)
    BioBERT
    XL-net 
    Electra
    ERNIE

Additional models may be added in future updates.
Evaluation Metrics

    F1 Score
    Hamming Loss
    Precision
    Recall
    ROC-AUC

Getting Started

Follow the instructions in the setup.md file to set up your environment and get the data ready for analysis.
Contributing

We welcome contributions to this project! Please see contributing.md for guidelines on how to contribute.
License

This project is licensed under the terms of the MIT license.
Contact

For any queries or discussions regarding this project, please open an issue in this repository.
Acknowledgements

Special thanks to all contributors and the PubMed database for providing the dataset used in this analysis.
